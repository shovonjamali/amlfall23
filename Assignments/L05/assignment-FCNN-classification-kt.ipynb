{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - classification\n",
    "\n",
    "Hi there! In this assignment, you will use a fully connected neural network (FCNN) to solve an adapted Question 1 of the winter 2023 exam in applied machine learning:\n",
    "\n",
    "As in Assignment 1, the primary objective of this exam is to perform image classification using the PCam dataset. For a detailed description of the dataset, please refer to the assignment 1 description. The assignment is posted as a Kaggle competition and is available here: https://www.kaggle.com/t/cda2949c5097437581cdb9abd32091ae\n",
    "\n",
    "To get you started, I have provided a complete working example, which is decent but not very impressive.\n",
    "\n",
    "When you are done, submit your results on the Kaggle webpage for this competition. If you do not like to show your score to everyone, you may use an anonymous username on Kaggle.\n",
    "\n",
    "However, I suggest you use your real name, after all it is just meant as an exercise and it is more fun that way. You can submit 5 times every day, so you can experiment with some stuff without being \"locked in\".\n",
    "\n",
    "# Details\n",
    "\n",
    "The metric used to score this assignment is accuracy (as in the first assignment).\n",
    "\n",
    "### Question (adapted from the exam):\n",
    "Use FCNN to perform image classification (tumor detection). Consider among other things the following:\n",
    "1. Different activation functions\n",
    "2. Different number of layers\n",
    "3. Different number of neurons in each layer\n",
    "4. Different learning rates\n",
    "5. Different batch sizes\n",
    "6. Different number of epochs\n",
    "7. Different optimizers\n",
    "\n",
    "**Note:** When you do hyperparameter tuning, you should use the validation set. The test set should only be used for the final evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints to get you started (with a very simple model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that takes a (None,96,96,3) array and turn it into (None, 32,32,1) (grayscale, resize and normalize). This function might also become handy if the original images are too large for your hardware configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_normalize_image(image):\n",
    "    image = tf.image.resize(image,[32,32])\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    return image / 255.0\n",
    "\n",
    "def convert_sample(data):\n",
    "\n",
    "# Create a TensorFlow dataset from the training data features\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# Define a function to resize each image in the dataset\n",
    "\n",
    "# Apply the resize function to each image in the dataset\n",
    "    resized_dataset = dataset.map(resize_and_normalize_image)\n",
    "\n",
    "# Convert the resized dataset to a NumPy array\n",
    "    resized_arr = np.array(list(resized_dataset.as_numpy_iterator()))\n",
    "\n",
    "    return resized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    #tf.keras.layers.Flatten(input_shape=(32,32,1)),\n",
    "    model.add(keras.layers.Flatten(input_shape=(32, 32, 1)))\n",
    "    \n",
    "    # Determine the number of hidden layers\n",
    "    #num_hidden_layers = hp.Int('num_hidden_layers', min_value=2, max_value=20)\n",
    "    \n",
    "    # Determine the number of nodes in each hidden layer\n",
    "#     for i in range(num_hidden_layers):\n",
    "#         model.add(keras.layers.Dense(\n",
    "#             units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "#             activation='relu'\n",
    "#         ))\n",
    "        \n",
    "    # Determine the number of nodes in each hidden layer\n",
    "    for i in range(hp.Int('num_hidden_layers', 2, 20)):         \n",
    "        #providing range for number of neurons in hidden layers\n",
    "        model.add(keras.layers.Dense(\n",
    "            units=hp.Int('num_of_neurons'+ str(i), min_value=32, max_value=512, step=32),\n",
    "            #activation='relu', # gave the best score so far\n",
    "            activation=hp.Choice('activation', values=['sigmoid','relu','tanh','elu','gelu','selu']),\n",
    "        ))\n",
    "\n",
    "    model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    # Choose an optimizer\n",
    "    #optimizer = hp.Choice('optimizer', values=['sgd', 'adam'])\n",
    "\n",
    "    # Choose a learning rate\n",
    "    # learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "#     if optimizer == 'sgd':\n",
    "#         momentum = hp.Float('momentum', min_value=0.0, max_value=1.0)\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer=SGD(\n",
    "#                 hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]), \n",
    "#                 momentum=0.9, \n",
    "#                 nesterov=True\n",
    "#             ),\n",
    "#             loss='sparse_categorical_crossentropy',\n",
    "#             metrics=['accuracy']\n",
    "#         )\n",
    "#     else:\n",
    "#         model.compile(\n",
    "#             optimizer=Adam(hp.Choice('learning_rate',values=[1e-2, 1e-3, 1e-4])),\n",
    "#             loss='sparse_categorical_crossentropy',\n",
    "#             metrics=['accuracy']\n",
    "#         )\n",
    "\n",
    "    model.compile(\n",
    "            optimizer=SGD(\n",
    "                hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]), \n",
    "                #hp.Choice('momentum', values=[0.1, 0.25, 0.5, 0.75, 0.9]), # did not help for the best one\n",
    "                nesterov=True\n",
    "            ),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "   )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Keras Tuner\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=4,\n",
    "    directory='keras_tuner',\n",
    "    project_name='a2_t5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 5\n",
      "num_hidden_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 20, 'step': 1, 'sampling': 'linear'}\n",
      "num_of_neurons0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "activation (Choice)\n",
      "{'default': 'sigmoid', 'conditions': [], 'values': ['sigmoid', 'relu', 'tanh', 'elu', 'gelu', 'selu'], 'ordered': False}\n",
      "num_of_neurons1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the raw training data: (26214, 96, 96, 3)\n",
      "Shape of the raw test data: (1638, 96, 96, 3)\n",
      "Shape the resized training data: (26214, 32, 32, 1)\n",
      "Shape the resized test data: (1638, 32, 32, 1)\n",
      "Shape of the raw labels: (26214, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the training data features\n",
    "X_train_raw = np.load('Xtrain.npy')\n",
    "print(f'Shape of the raw training data: {X_train_raw.shape}')\n",
    "X_test_raw = np.load('Xtest.npy')\n",
    "print(f'Shape of the raw test data: {X_test_raw.shape}')\n",
    "\n",
    "X = convert_sample(X_train_raw)\n",
    "print(f'Shape the resized training data: {X.shape}')\n",
    "\n",
    "X_test = convert_sample(X_test_raw)\n",
    "print(f'Shape the resized test data: {X_test.shape}')\n",
    "\n",
    "y = np.load('ytrain.npy')\n",
    "y = y.reshape(-1,1) \n",
    "print(f'Shape of the raw labels: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 04m 28s]\n",
      "val_accuracy: 0.50658018887043\n",
      "\n",
      "Best val_accuracy So Far: 0.7082300186157227\n",
      "Total elapsed time: 00h 29m 00s\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n",
      "Results summary\n",
      "Results in keras_tuner\\a2_t5\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 6\n",
      "num_of_neurons0: 224\n",
      "activation: selu\n",
      "num_of_neurons1: 128\n",
      "learning_rate: 0.01\n",
      "num_of_neurons2: 448\n",
      "num_of_neurons3: 96\n",
      "num_of_neurons4: 160\n",
      "num_of_neurons5: 96\n",
      "num_of_neurons6: 352\n",
      "num_of_neurons7: 96\n",
      "num_of_neurons8: 32\n",
      "num_of_neurons9: 128\n",
      "num_of_neurons10: 288\n",
      "num_of_neurons11: 192\n",
      "num_of_neurons12: 480\n",
      "num_of_neurons13: 480\n",
      "num_of_neurons14: 192\n",
      "num_of_neurons15: 96\n",
      "num_of_neurons16: 320\n",
      "Score: 0.7082300186157227\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 4\n",
      "num_of_neurons0: 480\n",
      "activation: elu\n",
      "num_of_neurons1: 352\n",
      "learning_rate: 0.01\n",
      "num_of_neurons2: 224\n",
      "num_of_neurons3: 320\n",
      "num_of_neurons4: 64\n",
      "num_of_neurons5: 256\n",
      "num_of_neurons6: 256\n",
      "num_of_neurons7: 256\n",
      "num_of_neurons8: 64\n",
      "num_of_neurons9: 448\n",
      "num_of_neurons10: 96\n",
      "num_of_neurons11: 256\n",
      "num_of_neurons12: 256\n",
      "num_of_neurons13: 320\n",
      "num_of_neurons14: 352\n",
      "num_of_neurons15: 480\n",
      "num_of_neurons16: 160\n",
      "Score: 0.689872220158577\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "num_of_neurons0: 288\n",
      "activation: selu\n",
      "num_of_neurons1: 512\n",
      "learning_rate: 0.01\n",
      "num_of_neurons2: 512\n",
      "num_of_neurons3: 96\n",
      "num_of_neurons4: 224\n",
      "num_of_neurons5: 64\n",
      "num_of_neurons6: 192\n",
      "num_of_neurons7: 352\n",
      "num_of_neurons8: 352\n",
      "num_of_neurons9: 320\n",
      "num_of_neurons10: 384\n",
      "num_of_neurons11: 352\n",
      "num_of_neurons12: 32\n",
      "num_of_neurons13: 320\n",
      "num_of_neurons14: 352\n",
      "num_of_neurons15: 64\n",
      "num_of_neurons16: 224\n",
      "Score: 0.6667461395263672\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 15\n",
      "num_of_neurons0: 128\n",
      "activation: tanh\n",
      "num_of_neurons1: 160\n",
      "learning_rate: 0.001\n",
      "num_of_neurons2: 32\n",
      "num_of_neurons3: 32\n",
      "num_of_neurons4: 32\n",
      "num_of_neurons5: 32\n",
      "num_of_neurons6: 32\n",
      "num_of_neurons7: 32\n",
      "num_of_neurons8: 32\n",
      "num_of_neurons9: 32\n",
      "num_of_neurons10: 32\n",
      "num_of_neurons11: 32\n",
      "num_of_neurons12: 32\n",
      "num_of_neurons13: 32\n",
      "num_of_neurons14: 32\n",
      "Score: 0.6350848823785782\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 5\n",
      "num_of_neurons0: 352\n",
      "activation: elu\n",
      "num_of_neurons1: 352\n",
      "learning_rate: 0.0001\n",
      "num_of_neurons2: 352\n",
      "num_of_neurons3: 480\n",
      "num_of_neurons4: 480\n",
      "num_of_neurons5: 448\n",
      "num_of_neurons6: 224\n",
      "num_of_neurons7: 480\n",
      "num_of_neurons8: 224\n",
      "num_of_neurons9: 128\n",
      "num_of_neurons10: 256\n",
      "num_of_neurons11: 96\n",
      "num_of_neurons12: 320\n",
      "num_of_neurons13: 256\n",
      "num_of_neurons14: 512\n",
      "num_of_neurons15: 320\n",
      "num_of_neurons16: 96\n",
      "Score: 0.5866393148899078\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 17\n",
      "num_of_neurons0: 320\n",
      "activation: tanh\n",
      "num_of_neurons1: 128\n",
      "learning_rate: 0.0001\n",
      "num_of_neurons2: 320\n",
      "num_of_neurons3: 448\n",
      "num_of_neurons4: 448\n",
      "num_of_neurons5: 320\n",
      "num_of_neurons6: 192\n",
      "num_of_neurons7: 128\n",
      "num_of_neurons8: 320\n",
      "num_of_neurons9: 320\n",
      "num_of_neurons10: 192\n",
      "num_of_neurons11: 256\n",
      "num_of_neurons12: 352\n",
      "num_of_neurons13: 32\n",
      "num_of_neurons14: 128\n",
      "num_of_neurons15: 32\n",
      "num_of_neurons16: 32\n",
      "Score: 0.5636563003063202\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "num_of_neurons0: 96\n",
      "activation: tanh\n",
      "num_of_neurons1: 32\n",
      "learning_rate: 0.0001\n",
      "num_of_neurons2: 352\n",
      "num_of_neurons3: 96\n",
      "num_of_neurons4: 96\n",
      "num_of_neurons5: 192\n",
      "num_of_neurons6: 224\n",
      "num_of_neurons7: 192\n",
      "num_of_neurons8: 224\n",
      "num_of_neurons9: 416\n",
      "num_of_neurons10: 128\n",
      "num_of_neurons11: 64\n",
      "num_of_neurons12: 128\n",
      "num_of_neurons13: 96\n",
      "num_of_neurons14: 480\n",
      "num_of_neurons15: 128\n",
      "num_of_neurons16: 288\n",
      "Score: 0.561224490404129\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 14\n",
      "num_of_neurons0: 64\n",
      "activation: tanh\n",
      "num_of_neurons1: 32\n",
      "learning_rate: 0.0001\n",
      "num_of_neurons2: 352\n",
      "num_of_neurons3: 256\n",
      "num_of_neurons4: 480\n",
      "num_of_neurons5: 416\n",
      "num_of_neurons6: 32\n",
      "num_of_neurons7: 96\n",
      "num_of_neurons8: 64\n",
      "num_of_neurons9: 416\n",
      "num_of_neurons10: 256\n",
      "num_of_neurons11: 128\n",
      "num_of_neurons12: 448\n",
      "num_of_neurons13: 192\n",
      "num_of_neurons14: 384\n",
      "num_of_neurons15: 64\n",
      "num_of_neurons16: 512\n",
      "Score: 0.5441541075706482\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 7\n",
      "num_of_neurons0: 416\n",
      "activation: sigmoid\n",
      "num_of_neurons1: 288\n",
      "learning_rate: 0.01\n",
      "num_of_neurons2: 416\n",
      "num_of_neurons3: 32\n",
      "num_of_neurons4: 288\n",
      "num_of_neurons5: 224\n",
      "num_of_neurons6: 320\n",
      "num_of_neurons7: 352\n",
      "num_of_neurons8: 480\n",
      "num_of_neurons9: 64\n",
      "num_of_neurons10: 224\n",
      "num_of_neurons11: 128\n",
      "num_of_neurons12: 160\n",
      "num_of_neurons13: 512\n",
      "num_of_neurons14: 512\n",
      "num_of_neurons15: 320\n",
      "num_of_neurons16: 256\n",
      "Score: 0.5069616436958313\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 13\n",
      "num_of_neurons0: 384\n",
      "activation: relu\n",
      "num_of_neurons1: 352\n",
      "learning_rate: 0.0001\n",
      "num_of_neurons2: 160\n",
      "num_of_neurons3: 160\n",
      "num_of_neurons4: 160\n",
      "num_of_neurons5: 192\n",
      "num_of_neurons6: 96\n",
      "num_of_neurons7: 352\n",
      "num_of_neurons8: 352\n",
      "num_of_neurons9: 32\n",
      "num_of_neurons10: 224\n",
      "num_of_neurons11: 384\n",
      "num_of_neurons12: 384\n",
      "num_of_neurons13: 352\n",
      "num_of_neurons14: 352\n",
      "num_of_neurons15: 224\n",
      "num_of_neurons16: 384\n",
      "Score: 0.50658018887043\n"
     ]
    }
   ],
   "source": [
    "# Search for the best hyperparameters\n",
    "\n",
    "# Define a batch size range to search within\n",
    "#batch_size = tuner.Int('batch_size', min_value=32, max_value=256, step=32)\n",
    "\n",
    "#tuner.search(X_train, y_train, epochs=10, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "best_model = tuner.get_best_models()[0]\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Hyperparameters:\n",
      "Number of Hidden Layers: 6\n",
      "Hidden Layer 1 Units: 224\n",
      "Hidden Layer 1 Activation: selu\n",
      "Hidden Layer 2 Units: 128\n",
      "Hidden Layer 2 Activation: selu\n",
      "Hidden Layer 3 Units: 448\n",
      "Hidden Layer 3 Activation: selu\n",
      "Hidden Layer 4 Units: 96\n",
      "Hidden Layer 4 Activation: selu\n",
      "Hidden Layer 5 Units: 160\n",
      "Hidden Layer 5 Activation: selu\n",
      "Hidden Layer 6 Units: 96\n",
      "Hidden Layer 6 Activation: selu\n",
      "Learning Rate: 0.01\n",
      "Optimal Accuracy: 0.7082300186157227\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_accuracy = tuner.oracle.get_best_trials(num_trials=1)[0].score\n",
    "\n",
    "print(\"Optimal Hyperparameters:\")\n",
    "\n",
    "print(f\"Number of Hidden Layers: {best_hps.get('num_hidden_layers')}\")\n",
    "for i in range(best_hps.get('num_hidden_layers')):\n",
    "    print(f\"Hidden Layer {i+1} Units: {best_hps.get(f'num_of_neurons{i}')}\")\n",
    "    print(f\"Hidden Layer {i+1} Activation: {best_hps.get('activation')}\")\n",
    "    \n",
    "#print(f\"Optimizer: {best_hps.get('optimizer')}\")\n",
    "\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "#print(f\"Momentum: {best_hps.get('momentum')}\")\n",
    "\n",
    "print(f\"Optimal Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final model with the best hyperparameters\n",
    "final_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 0.6787 - accuracy: 0.5968\n",
      "Epoch 2/10\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.6178 - accuracy: 0.6566\n",
      "Epoch 3/10\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.6002 - accuracy: 0.6743\n",
      "Epoch 4/10\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.5909 - accuracy: 0.6891\n",
      "Epoch 5/10\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.5815 - accuracy: 0.6938\n",
      "Epoch 6/10\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.5706 - accuracy: 0.7070\n",
      "Epoch 7/10\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.5676 - accuracy: 0.7097\n",
      "Epoch 8/10\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.5536 - accuracy: 0.7199\n",
      "Epoch 9/10\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.5465 - accuracy: 0.7248\n",
      "Epoch 10/10\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.5372 - accuracy: 0.7291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2590b9ffd00>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the final model and save the predictions\n",
    "final_model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code makes predictions and then saves them (after checking they are in correct format).\n",
    "\n",
    "The argmax converts probabilities to specific class predictions.\n",
    "\n",
    "And finally convert to appropriate $\\texttt{.csv}$ for Kaggle submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model with the best hyperparameter and then search for momentum\n",
    "# def build_model_with_batch(learning_rate, momentum):\n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.Flatten(input_shape=(32, 32, 1)),\n",
    "#         tf.keras.layers.Dense(224, activation='relu'),\n",
    "#         tf.keras.layers.Dense(224, activation='relu'),\n",
    "#         tf.keras.layers.Dense(32, activation='relu'),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "#         tf.keras.layers.Dense(32, activation='relu'),\n",
    "#         tf.keras.layers.Dense(32, activation='relu'),\n",
    "#         tf.keras.layers.Dense(32, activation='relu'),\n",
    "#         tf.keras.layers.Dense(10, activation='softmax'),\n",
    "#     ])\n",
    "    \n",
    "#     optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    \n",
    "#     model.compile(\n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#         optimizer=optimizer,\n",
    "#         metrics=['accuracy'],\n",
    "#     )\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.0594 - accuracy: 0.4701\n",
      "Epoch 2/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6965 - accuracy: 0.5017\n",
      "Epoch 3/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6931 - accuracy: 0.5034\n",
      "Epoch 4/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6916 - accuracy: 0.5080\n",
      "Epoch 5/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6898 - accuracy: 0.5138\n",
      "Epoch 6/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6892 - accuracy: 0.5139\n",
      "Epoch 7/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6872 - accuracy: 0.5216\n",
      "Epoch 8/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6859 - accuracy: 0.5285\n",
      "Epoch 9/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6858 - accuracy: 0.5328\n",
      "Epoch 10/10\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.6844 - accuracy: 0.5320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x259007aaaa0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the optimal model with the best hyperparameters\n",
    "# learning_rate = 0.001\n",
    "# momentum = 0.9\n",
    "# batch_size = 32\n",
    "\n",
    "# learning_rate = 0.0001\n",
    "# momentum = 0.9\n",
    "# batch_size = 32\n",
    "\n",
    "# optimal_model = build_model_with_batch(learning_rate=learning_rate, momentum=momentum) \n",
    "\n",
    "# # Train the optimal model and save the predictions\n",
    "# optimal_model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# #learning_rates = [0.1, 0.01, 0.001, 0.00001] # must be positive floats. Default depends on optimizer\n",
    "# batch_sizes = [8,16,32,64,128] # # must be positive ints. Default is 32\n",
    "# #momentums = [0.1,0.25,0.5,0.75,0.9] # must be in [0, 1). Default (for SGD) is 0.0\n",
    "# learning_rate = 0.001\n",
    "# momentum = 0.9\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # for learning_rate in tqdm(learning_rates):\n",
    "# for batch_size in batch_sizes:\n",
    "#     #for momentum in momentums:\n",
    "#         # model = build_model_with_momentum(learning_rate, momentum=momentum) \n",
    "#     model = build_model_with_batch(learning_rate=learning_rate, momentum=momentum) \n",
    "#     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=batch_size, verbose=1)# remember to pass in batch_size here! Also remember to use epochs=2\n",
    "#     loss, acc = model.evaluate(x_test, y_test)\n",
    "#     results.append((acc, batch_size))\n",
    "    \n",
    "# results = pd.DataFrame(results, columns=['Accuracy', 'Batch size'])\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[results['Accuracy'] == results['Accuracy'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate final model.\n",
    "# Remember to use both train and val data for training for best performance! \n",
    "# Similar to what we have done in all the other exercises/assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = final_model.predict(X_test)\n",
    "y_test_hat = np.argmax(y_test_hat, axis=1)\n",
    "\n",
    "ytest_hat_pd = pd.DataFrame({\n",
    "    'Id': list(range(len(y_test_hat))),\n",
    "    'Predicted': y_test_hat.reshape(-1,),\n",
    "})\n",
    "\n",
    "ytest_hat_pd.to_csv('y_test_hat_fcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3cedec8935a2c28d6fd602c3007747750e2af1c4c937c29fac0d323bf1b544b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
