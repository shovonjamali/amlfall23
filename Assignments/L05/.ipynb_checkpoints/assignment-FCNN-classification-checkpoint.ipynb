{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - classification\n",
    "\n",
    "Hi there! In this assignment, you will use a fully connected neural network (FCNN) to solve an adapted Question 1 of the winter 2023 exam in applied machine learning:\n",
    "\n",
    "As in Assignment 1, the primary objective of this exam is to perform image classification using the PCam dataset. For a detailed description of the dataset, please refer to the assignment 1 description. The assignment is posted as a Kaggle competition and is available here: https://www.kaggle.com/t/cda2949c5097437581cdb9abd32091ae\n",
    "\n",
    "To get you started, I have provided a complete working example, which is decent but not very impressive.\n",
    "\n",
    "When you are done, submit your results on the Kaggle webpage for this competition. If you do not like to show your score to everyone, you may use an anonymous username on Kaggle.\n",
    "\n",
    "However, I suggest you use your real name, after all it is just meant as an exercise and it is more fun that way. You can submit 5 times every day, so you can experiment with some stuff without being \"locked in\".\n",
    "\n",
    "# Details\n",
    "\n",
    "The metric used to score this assignment is accuracy (as in the first assignment).\n",
    "\n",
    "### Question (adapted from the exam):\n",
    "Use FCNN to perform image classification (tumor detection). Consider among other things the following:\n",
    "1. Different activation functions\n",
    "2. Different number of layers\n",
    "3. Different number of neurons in each layer\n",
    "4. Different learning rates\n",
    "5. Different batch sizes\n",
    "6. Different number of epochs\n",
    "7. Different optimizers\n",
    "\n",
    "**Note:** When you do hyperparameter tuning, you should use the validation set. The test set should only be used for the final evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints to get you started (with a very simple model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that takes a (None,96,96,3) array and turn it into (None, 32,32,1) (grayscale, resize and normalize). This function might also become handy if the original images are too large for your hardware configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_normalize_image(image):\n",
    "    image = tf.image.resize(image,[32,32])\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    return image / 255.0\n",
    "\n",
    "def convert_sample(data):\n",
    "\n",
    "# Create a TensorFlow dataset from the training data features\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# Define a function to resize each image in the dataset\n",
    "\n",
    "# Apply the resize function to each image in the dataset\n",
    "    resized_dataset = dataset.map(resize_and_normalize_image)\n",
    "\n",
    "# Convert the resized dataset to a NumPy array\n",
    "    resized_arr = np.array(list(resized_dataset.as_numpy_iterator()))\n",
    "\n",
    "    return resized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the raw training data: (26214, 96, 96, 3)\n",
      "Shape of the raw test data: (1638, 96, 96, 3)\n",
      "Shape the resized training data: (26214, 32, 32, 1)\n",
      "Shape the resized test data: (1638, 32, 32, 1)\n",
      "Shape of the raw labels: (26214, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the training data features\n",
    "X_train_raw = np.load('Xtrain.npy')\n",
    "print(f'Shape of the raw training data: {X_train_raw.shape}')\n",
    "X_test_raw = np.load('Xtest.npy')\n",
    "print(f'Shape of the raw test data: {X_test_raw.shape}')\n",
    "\n",
    "X_train = convert_sample(X_train_raw)\n",
    "print(f'Shape the resized training data: {X_train.shape}')\n",
    "\n",
    "X_test = convert_sample(X_test_raw)\n",
    "print(f'Shape the resized test data: {X_test.shape}')\n",
    "\n",
    "y_raw = np.load('ytrain.npy')\n",
    "y_raw = y_raw.reshape(-1,1) \n",
    "print(f'Shape of the raw labels: {y_raw.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.7372 - accuracy: 0.4964\n",
      "Epoch 2/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6939 - accuracy: 0.4960\n",
      "Epoch 3/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.5051\n",
      "Epoch 4/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6936 - accuracy: 0.4958\n",
      "Epoch 5/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 6/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.5008\n",
      "Epoch 7/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6936 - accuracy: 0.4924\n",
      "Epoch 8/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4962\n",
      "Epoch 9/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4961\n",
      "Epoch 10/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.5010\n",
      "Epoch 11/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4964\n",
      "Epoch 12/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.5006\n",
      "Epoch 13/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4960\n",
      "Epoch 14/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.5005\n",
      "Epoch 15/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4999\n",
      "Epoch 16/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "Epoch 17/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.5060\n",
      "Epoch 18/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.5002\n",
      "Epoch 19/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.5022\n",
      "Epoch 20/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.5027\n",
      "Epoch 21/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4982\n",
      "Epoch 22/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.5028\n",
      "Epoch 23/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6935 - accuracy: 0.4945\n",
      "Epoch 24/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6934 - accuracy: 0.4948\n",
      "Epoch 25/25\n",
      "410/410 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.5011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f15f9eda0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(32,32,1)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "sgd_opt = SGD(learning_rate=0.01, momentum=0.9, nesterov=True) # oob: learning_rate=0.01, momentum=0.9, nesterov=True\n",
    "#adam_opt = Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=adam_opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_raw, epochs=25, batch_size=64, verbose=1) # oob: epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code makes predictions and then saves them (after checking they are in correct format).\n",
    "\n",
    "The argmax converts probabilities to specific class predictions.\n",
    "\n",
    "And finally convert to appropriate $\\texttt{.csv}$ for Kaggle submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model.predict(X_test)\n",
    "y_test_hat = np.argmax(y_test_hat, axis=1)\n",
    "\n",
    "ytest_hat_pd = pd.DataFrame({\n",
    "    'Id': list(range(len(y_test_hat))),\n",
    "    'Predicted': y_test_hat.reshape(-1,),\n",
    "})\n",
    "\n",
    "ytest_hat_pd.to_csv('y_test_hat_fcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3cedec8935a2c28d6fd602c3007747750e2af1c4c937c29fac0d323bf1b544b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
